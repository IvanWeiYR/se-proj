{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12)\n",
    "num_observations = 5000\n",
    "\n",
    "x1 = np.random.multivariate_normal([0, 0], [[2, .75],[.75, 2]], num_observations)\n",
    "x2 = np.random.multivariate_normal([1, 4], [[1, .75],[.75, 1]], num_observations)\n",
    "x3 = np.random.multivariate_normal([2, 8], [[0, .75],[.75, 0]], num_observations)\n",
    "\n",
    "simulated_separableish_features = np.vstack((x1, x2, x3)).astype(np.float32)\n",
    "print (simulated_separableish_features.shape)\n",
    "print (simulated_separableish_features)\n",
    "simulated_labels = np.hstack((np.zeros(num_observations),\n",
    "                                np.ones(num_observations), np.ones(num_observations) + 1))\n",
    "print (simulated_labels.shape)\n",
    "print (simulated_labels)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.scatter(simulated_separableish_features[:, 0], simulated_separableish_features[:, 1],\n",
    "            c = simulated_labels, alpha = .4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformat the input labels to one-hot encoded vector\n",
    "labels_onehot = np.zeros((simulated_labels.shape[0], 3)).astype(int)\n",
    "labels_onehot[np.arange(len(simulated_labels)), simulated_labels.astype(int)] = 1\n",
    "\n",
    "train_dataset, test_dataset, \\\n",
    "train_labels, test_labels = train_test_split(\n",
    "    simulated_separableish_features, labels_onehot, test_size = .1, random_state = 12)\n",
    "print (train_dataset.shape)\n",
    "print (train_dataset)\n",
    "print (test_dataset.shape)\n",
    "print (test_dataset)\n",
    "print (train_labels.shape)\n",
    "print (train_labels)\n",
    "print (test_labels.shape)\n",
    "print (test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 0 1 0]\n",
      "[[0 0 1 0 0 1 1 1 0]\n",
      " [1 1 1 1 0 1 0 0 1]\n",
      " [0 0 0 0 0 1 1 0 0]\n",
      " [1 0 0 0 1 1 1 1 0]\n",
      " [1 1 1 1 0 1 0 1 1]\n",
      " [1 1 0 0 1 0 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# in this case; num_features = num_statements\n",
    "# num_input = num_test_cases\n",
    "dataset = np.matrix([\n",
    "    [1,1,1,1,0,1,0,0,1],\n",
    "    [1,0,0,0,1,1,1,1,0],\n",
    "    [0,0,0,0,0,1,1,0,0],\n",
    "    [1,1,0,0,1,0,1,1,1],\n",
    "    [1,1,1,0,1,1,1,1,1],\n",
    "    [0,0,1,0,0,1,1,1,0],\n",
    "    [1,1,1,1,0,1,0,1,1]\n",
    "]).astype(int)\n",
    "# in labels, 0 means success, 1 means failure\n",
    "#labels = np.array([[0],[0],[0],[0],[0],[1],[1]])\n",
    "labels = np.array([0,0,0,0,0,1,1])\n",
    "# transform the labels to one-hot format\n",
    "labels_onehot = np.zeros((labels.shape[0], 2)).astype(int)\n",
    "labels_onehot[np.arange(len(labels)), labels.astype(int)] = 1\n",
    "# divide the dataset into train and test datasets\n",
    "train_dataset, test_dataset, \\\n",
    "train_labels, test_labels = train_test_split(\n",
    "    dataset, labels, test_size = .1, random_state = 12)\n",
    "print (train_labels)\n",
    "print (train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build_model\n",
    "hidden_nodes = 5\n",
    "# num_labels is 1, only true/false classification\n",
    "num_labels = 1\n",
    "batch_size = 100\n",
    "# num_features is num_statements in this case\n",
    "num_features = train_dataset.shape[1]\n",
    "learning_rate = .01\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Data\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape = [None, num_features])\n",
    "    tf_train_labels = tf.placeholder(tf.float32, None)\n",
    "    tf_test_dataset = tf.constant(test_dataset, dtype=tf.float32)\n",
    "  \n",
    "    # Weights and Biases\n",
    "    layer1_weights = tf.Variable(tf.truncated_normal([num_features, hidden_nodes]))\n",
    "    layer1_biases = tf.Variable(tf.zeros([hidden_nodes]))\n",
    "    \n",
    "    layer2_weights = tf.Variable(tf.truncated_normal([hidden_nodes, num_labels]))\n",
    "    layer2_biases = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    # Three-Layer Network\n",
    "    def three_layer_network(data):\n",
    "        input_layer = tf.matmul(data, layer1_weights)\n",
    "        hidden = input_layer + layer1_biases\n",
    "        output_layer = tf.nn.sigmoid(tf.matmul(hidden, layer2_weights) + layer2_biases)\n",
    "        return output_layer\n",
    "    \n",
    "    # Model Scores\n",
    "    model_scores = three_layer_network(tf_train_dataset)\n",
    "    model_scores_t = tf.reshape(model_scores,[-1])\n",
    "    \n",
    "    # Loss\n",
    "    # cross_entropy = tf.reduce_sum(- tf_train_labels * tf.log(model_scores_t))\n",
    "    cross_entropy = tf.reduce_sum(tf.square(tf_train_labels - model_scores_t))\n",
    "    loss = tf.reduce_mean(cross_entropy)\n",
    "    #loss = tf.nn.softmax_cross_entropy_with_logits(logits=model_scores_t, labels=tf_train_labels)\n",
    "  \n",
    "    # Optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "    \n",
    "    # Predictions\n",
    "    #train_prediction = tf.nn.sigmoid(model_scores)\n",
    "    train_prediction = model_scores\n",
    "    #test_prediction = tf.nn.sigmoid(three_layer_network(tf_test_dataset))\n",
    "    test_prediction = tf.reshape(three_layer_network(tf_test_dataset),[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  1.]]\n",
      "(9, 9)\n"
     ]
    }
   ],
   "source": [
    "# PART B\n",
    "# estimate the suspiciousness of each statement\n",
    "test_susp_dataset = np.identity(dataset.shape[1])\n",
    "print (test_susp_dataset)\n",
    "print (test_susp_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    preds_correct_boolean =  np.argmax(predictions, 1) == np.argmax(labels, 1)\n",
    "    correct_predictions = np.sum(preds_correct_boolean)\n",
    "    accuracy = 100.0 * correct_predictions / predictions.shape[0]\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 0: 1.45251369476 - [ 0.4408195   0.04764955  0.32713005  0.36614913  0.07790515  0.21499357]\n",
      "Minibatch loss at step 1000: 0.0184685122222 - [ 0.95740771  0.08134051  0.02175828  0.04942956  0.91748554  0.01768586]\n",
      "Minibatch loss at step 2000: 0.0071024550125 - [ 0.97555315  0.05149345  0.01090244  0.03074255  0.94817978  0.01019452]\n",
      "Minibatch loss at step 3000: 0.00416782777756 - [ 0.98194075  0.03976906  0.00742066  0.02357069  0.96010703  0.00761707]\n",
      "Minibatch loss at step 4000: 0.00288299331442 - [ 0.98530072  0.03316745  0.00568622  0.0195851   0.96665514  0.00624943]\n",
      "Minibatch loss at step 5000: 0.00217740796506 - [ 0.98742491  0.02891739  0.00464405  0.01701338  0.9709664   0.00539327]\n",
      "Minibatch loss at step 6000: 0.000698460731655 - [ 0.97400969  0.00479215]\n",
      "Minibatch loss at step 7000: 0.00143625540659 - [ 0.99000311  0.02356232  0.00344051  0.01380553  0.97634     0.00434843]\n",
      "Minibatch loss at step 8000: 0.00122020719573 - [ 0.99085581  0.02173358  0.00305885  0.01271655  0.97815555  0.00399946]\n",
      "Minibatch loss at step 9000: 0.00105785776395 - [ 0.99154305  0.02026166  0.00275984  0.01183617  0.9796468   0.00372007]\n",
      "Minibatch loss at step 10000: 0.000931706279516 - [ 0.99211019  0.0190379   0.00251872  0.01110606  0.98089117  0.00348992]\n",
      "[ 0.0655753]\n",
      "[0]\n",
      "0\n",
      "[[ 1.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "[array([ 0.00054048], dtype=float32)]\n",
      "1\n",
      "[[ 0.  1.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "[array([ 0.52034241], dtype=float32)]\n",
      "2\n",
      "[[ 0.  0.  1.  0.  0.  0.  0.  0.  0.]]\n",
      "[array([ 0.88323665], dtype=float32)]\n",
      "3\n",
      "[[ 0.  0.  0.  1.  0.  0.  0.  0.  0.]]\n",
      "[array([ 0.68429756], dtype=float32)]\n",
      "4\n",
      "[[ 0.  0.  0.  0.  1.  0.  0.  0.  0.]]\n",
      "[array([ 0.33415392], dtype=float32)]\n",
      "5\n",
      "[[ 0.  0.  0.  0.  0.  1.  0.  0.  0.]]\n",
      "[array([ 0.29952726], dtype=float32)]\n",
      "6\n",
      "[[ 0.  0.  0.  0.  0.  0.  1.  0.  0.]]\n",
      "[array([ 0.00236657], dtype=float32)]\n",
      "7\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  1.  0.]]\n",
      "[array([ 0.99905998], dtype=float32)]\n",
      "8\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  1.]]\n",
      "[array([ 0.04705785], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        minibatch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        minibatch_labels = train_labels[offset:(offset + batch_size)]\n",
    "        feed_dict = {tf_train_dataset : minibatch_data, tf_train_labels : minibatch_labels}\n",
    "        #session.run(tf.global_variables_initializer())\n",
    "        \n",
    "        _, l, predictions = session.run(\n",
    "            [optimizer, loss, model_scores_t], feed_dict = feed_dict\n",
    "            )\n",
    "        \n",
    "        if step % 1000 == 0:\n",
    "            # print (session.run(train_prediction))\n",
    "            print 'Minibatch loss at step {0}: {1} - {2}'.format(step, l, predictions)\n",
    "\n",
    "    print test_prediction.eval()\n",
    "    print test_labels\n",
    "    # print 'Test accuracy: {0}%'.format(accuracy(test_prediction.eval(), test_labels))\n",
    "    for count in range(dataset.shape[1]):\n",
    "        print (count)\n",
    "        print (test_susp_dataset[count,:].reshape(1,dataset.shape[1]))\n",
    "        feed_dict = {tf_test_dataset: test_susp_dataset[count,:].reshape(1,dataset.shape[1])}\n",
    "        predictions = session.run([test_prediction], feed_dict)\n",
    "        print (predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.794377\n",
      "[[ 0.76852483]\n",
      " [ 0.71094948]\n",
      " [ 0.59868765]]\n",
      "0.105361\n",
      "[ 0.89999998  0.40000001  0.40000001]\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([[0.9],[0.4],[0.4]])\n",
    "anew = tf.constant([0.9,0.4,0.4])\n",
    "b = tf.constant([[1],[0],[0]])\n",
    "bnew = tf.constant([1,0,0],dtype=tf.float32)\n",
    "result =  tf.nn.softmax_cross_entropy_with_logits(logits=a,labels=b,dim=0)[0]\n",
    "result2 = tf.nn.sigmoid([[1.2],[0.9],[0.4]])\n",
    "result3 = tf.reduce_sum(-bnew*tf.log(anew))\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "print result.eval()\n",
    "print result2.eval()\n",
    "print result3.eval()\n",
    "\n",
    "anew = tf.reshape(a,[-1])\n",
    "print anew.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
